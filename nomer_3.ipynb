{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIfEXfiLvxoa",
        "outputId": "3e2bbf28-88ca-421f-ef5c-9efa14aea6e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: BeautifulSoup4 in /usr/local/lib/python3.10/dist-packages (4.12.2)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.19.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx) (2023.7.22)\n",
            "Requirement already satisfied: httpcore<0.19.0,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from httpx) (0.18.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from BeautifulSoup4) (2.5)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.19.0,>=0.18.0->httpx) (3.7.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.19.0,>=0.18.0->httpx) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.19.0,>=0.18.0->httpx) (1.1.3)\n"
          ]
        }
      ],
      "source": [
        "pip install httpx BeautifulSoup4 polars tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade httpx beautifulsoup4 tqdm polars\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-wB7lABxa0_",
        "outputId": "765225eb-60f5-4431-e563-855746b230d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.19.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx) (2023.7.22)\n",
            "Requirement already satisfied: httpcore<0.19.0,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from httpx) (0.18.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.19.0,>=0.18.0->httpx) (3.7.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.19.0,>=0.18.0->httpx) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.19.0,>=0.18.0->httpx) (1.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade asyncio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X2elMfKxeus",
        "outputId": "ef6d6f81-5e58-4588-b39c-aaf11760f353"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: asyncio in /usr/local/lib/python3.10/dist-packages (3.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import httpx\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import polars as pl\n",
        "\n",
        "# Define the base URL and the maximum number of pages for each level\n",
        "base_url = \"https://www.fortiguard.com/encyclopedia?type=ips&risk={level}&page={page}\"\n",
        "max_pages = [5, 10, 15, 20, 25]  # Define the maximum number of pages for each level\n",
        "\n",
        "def scrape_page(url):\n",
        "    try:\n",
        "        response = httpx.get(url)\n",
        "        response.raise_for_status()\n",
        "        html_content = response.text\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        title = soup.find('title').text  # Replace with actual HTML elements for title\n",
        "        link = url  # Replace with actual HTML elements for link\n",
        "        return {\"title\": title, \"link\": link}\n",
        "    except Exception as e:\n",
        "        print(f\"Error while scraping {url}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def scrape_level(level, max_page):\n",
        "    scraped_data = []\n",
        "\n",
        "    for page in range(1, max_page + 1):\n",
        "        url = base_url.format(level=level, page=page)\n",
        "        result = scrape_page(url)\n",
        "        if result:\n",
        "            scraped_data.append(result)\n",
        "\n",
        "    return scraped_data\n",
        "\n",
        "def main():\n",
        "    dataset_dir = \"datasets\"\n",
        "    skipped = {}\n",
        "    os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "    for level, max_page in enumerate(max_pages, start=1):\n",
        "        print(f\"Scraping Level {level}\")\n",
        "        scraped_data = scrape_level(level, max_page)\n",
        "\n",
        "        if scraped_data:\n",
        "            df = pl.DataFrame(scraped_data)\n",
        "            csv_file = os.path.join(dataset_dir, f\"forti_lists_{level}.csv\")\n",
        "            df.write_csv(csv_file)\n",
        "\n",
        "        skipped[level] = [i for i in range(1, max_page + 1) if i not in range(1, len(scraped_data) + 1)]\n",
        "\n",
        "    with open(os.path.join(dataset_dir, 'skipped.json'), 'w') as json_file:\n",
        "        json.dump(skipped, json_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhZh4ac9v3gD",
        "outputId": "a4164f4b-f1b1-4814-f669-8b9bfd92119e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping Level 1\n",
            "Scraping Level 2\n",
            "Scraping Level 3\n",
            "Scraping Level 4\n",
            "Scraping Level 5\n"
          ]
        }
      ]
    }
  ]
}